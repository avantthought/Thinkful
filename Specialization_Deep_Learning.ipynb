{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 177, 187],\n",
       "         [126, 137, 136],\n",
       "         [105, 104,  95],\n",
       "         ...,\n",
       "         [ 91,  95,  71],\n",
       "         [ 87,  90,  71],\n",
       "         [ 79,  81,  70]],\n",
       "\n",
       "        [[140, 160, 169],\n",
       "         [145, 153, 154],\n",
       "         [125, 125, 118],\n",
       "         ...,\n",
       "         [ 96,  99,  78],\n",
       "         [ 77,  80,  62],\n",
       "         [ 71,  73,  61]],\n",
       "\n",
       "        [[140, 155, 164],\n",
       "         [139, 146, 149],\n",
       "         [115, 115, 112],\n",
       "         ...,\n",
       "         [ 79,  82,  64],\n",
       "         [ 68,  70,  55],\n",
       "         [ 67,  69,  55]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[175, 167, 166],\n",
       "         [156, 154, 160],\n",
       "         [154, 160, 170],\n",
       "         ...,\n",
       "         [ 42,  34,  36],\n",
       "         [ 61,  53,  57],\n",
       "         [ 93,  83,  91]],\n",
       "\n",
       "        [[165, 154, 128],\n",
       "         [156, 152, 130],\n",
       "         [159, 161, 142],\n",
       "         ...,\n",
       "         [103,  93,  96],\n",
       "         [123, 114, 120],\n",
       "         [131, 121, 131]],\n",
       "\n",
       "        [[163, 148, 120],\n",
       "         [158, 148, 122],\n",
       "         [163, 156, 133],\n",
       "         ...,\n",
       "         [143, 133, 139],\n",
       "         [143, 134, 142],\n",
       "         [143, 133, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[113, 120, 112],\n",
       "         [111, 118, 111],\n",
       "         [105, 112, 106],\n",
       "         ...,\n",
       "         [ 72,  81,  80],\n",
       "         [ 72,  80,  79],\n",
       "         [ 72,  80,  79]],\n",
       "\n",
       "        [[111, 118, 110],\n",
       "         [104, 111, 104],\n",
       "         [ 99, 106,  98],\n",
       "         ...,\n",
       "         [ 68,  75,  73],\n",
       "         [ 70,  76,  75],\n",
       "         [ 78,  84,  82]],\n",
       "\n",
       "        [[106, 113, 105],\n",
       "         [ 99, 106,  98],\n",
       "         [ 95, 102,  94],\n",
       "         ...,\n",
       "         [ 78,  85,  83],\n",
       "         [ 79,  85,  83],\n",
       "         [ 80,  86,  84]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 35, 178, 235],\n",
       "         [ 40, 176, 239],\n",
       "         [ 42, 176, 241],\n",
       "         ...,\n",
       "         [ 99, 177, 219],\n",
       "         [ 79, 147, 197],\n",
       "         [ 89, 148, 189]],\n",
       "\n",
       "        [[ 57, 182, 234],\n",
       "         [ 44, 184, 250],\n",
       "         [ 50, 183, 240],\n",
       "         ...,\n",
       "         [156, 182, 200],\n",
       "         [141, 177, 206],\n",
       "         [116, 149, 175]],\n",
       "\n",
       "        [[ 98, 197, 237],\n",
       "         [ 64, 189, 252],\n",
       "         [ 69, 192, 245],\n",
       "         ...,\n",
       "         [188, 195, 206],\n",
       "         [119, 135, 147],\n",
       "         [ 61,  79,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 73,  79,  77],\n",
       "         [ 53,  63,  68],\n",
       "         [ 54,  68,  80],\n",
       "         ...,\n",
       "         [ 17,  40,  64],\n",
       "         [ 21,  36,  51],\n",
       "         [ 33,  48,  49]],\n",
       "\n",
       "        [[ 61,  68,  75],\n",
       "         [ 55,  70,  86],\n",
       "         [ 57,  79, 103],\n",
       "         ...,\n",
       "         [ 24,  48,  72],\n",
       "         [ 17,  35,  53],\n",
       "         [  7,  23,  32]],\n",
       "\n",
       "        [[ 44,  56,  73],\n",
       "         [ 46,  66,  88],\n",
       "         [ 49,  77, 105],\n",
       "         ...,\n",
       "         [ 27,  52,  77],\n",
       "         [ 21,  43,  66],\n",
       "         [ 12,  31,  50]]],\n",
       "\n",
       "\n",
       "       [[[189, 211, 240],\n",
       "         [186, 208, 236],\n",
       "         [185, 207, 235],\n",
       "         ...,\n",
       "         [175, 195, 224],\n",
       "         [172, 194, 222],\n",
       "         [169, 194, 220]],\n",
       "\n",
       "        [[194, 210, 239],\n",
       "         [191, 207, 236],\n",
       "         [190, 206, 235],\n",
       "         ...,\n",
       "         [173, 192, 220],\n",
       "         [171, 191, 218],\n",
       "         [167, 190, 216]],\n",
       "\n",
       "        [[208, 219, 244],\n",
       "         [205, 216, 240],\n",
       "         [204, 215, 239],\n",
       "         ...,\n",
       "         [175, 191, 217],\n",
       "         [172, 190, 216],\n",
       "         [169, 191, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[207, 199, 181],\n",
       "         [203, 195, 175],\n",
       "         [203, 196, 173],\n",
       "         ...,\n",
       "         [135, 132, 127],\n",
       "         [162, 158, 150],\n",
       "         [168, 163, 151]],\n",
       "\n",
       "        [[198, 190, 170],\n",
       "         [189, 181, 159],\n",
       "         [180, 172, 147],\n",
       "         ...,\n",
       "         [178, 171, 160],\n",
       "         [175, 169, 156],\n",
       "         [175, 169, 154]],\n",
       "\n",
       "        [[198, 189, 173],\n",
       "         [189, 181, 162],\n",
       "         [178, 170, 149],\n",
       "         ...,\n",
       "         [195, 184, 169],\n",
       "         [196, 189, 171],\n",
       "         [195, 190, 171]]],\n",
       "\n",
       "\n",
       "       [[[229, 229, 239],\n",
       "         [236, 237, 247],\n",
       "         [234, 236, 247],\n",
       "         ...,\n",
       "         [217, 219, 233],\n",
       "         [221, 223, 234],\n",
       "         [222, 223, 233]],\n",
       "\n",
       "        [[222, 221, 229],\n",
       "         [239, 239, 249],\n",
       "         [233, 234, 246],\n",
       "         ...,\n",
       "         [223, 223, 236],\n",
       "         [227, 228, 238],\n",
       "         [210, 211, 220]],\n",
       "\n",
       "        [[213, 206, 211],\n",
       "         [234, 232, 239],\n",
       "         [231, 233, 244],\n",
       "         ...,\n",
       "         [220, 220, 232],\n",
       "         [220, 219, 232],\n",
       "         [202, 203, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[150, 143, 135],\n",
       "         [140, 135, 127],\n",
       "         [132, 127, 120],\n",
       "         ...,\n",
       "         [224, 222, 218],\n",
       "         [230, 228, 225],\n",
       "         [241, 241, 238]],\n",
       "\n",
       "        [[137, 132, 126],\n",
       "         [130, 127, 120],\n",
       "         [125, 121, 115],\n",
       "         ...,\n",
       "         [181, 180, 178],\n",
       "         [202, 201, 198],\n",
       "         [212, 211, 207]],\n",
       "\n",
       "        [[122, 119, 114],\n",
       "         [118, 116, 110],\n",
       "         [120, 116, 111],\n",
       "         ...,\n",
       "         [179, 177, 173],\n",
       "         [164, 164, 162],\n",
       "         [163, 163, 161]]]], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change shape \n",
    "# Note that our images are 3*32*32 pixels, so in reshaping to arrays we want\n",
    "# 50,000 arrays of length 3072, one for each image\n",
    "x_train = x_train.reshape(50000, 3072)\n",
    "x_test = x_test.reshape(10000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 201,482\n",
      "Trainable params: 201,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 2.0981 - acc: 0.2250 - val_loss: 1.8931 - val_acc: 0.3123\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.9106 - acc: 0.3002 - val_loss: 1.8251 - val_acc: 0.3333\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 1.8557 - acc: 0.3221 - val_loss: 1.7447 - val_acc: 0.3735\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.8245 - acc: 0.3359 - val_loss: 1.7650 - val_acc: 0.3615\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 1.8023 - acc: 0.3471 - val_loss: 1.7390 - val_acc: 0.3853\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 4s 86us/step - loss: 1.7784 - acc: 0.3566 - val_loss: 1.7074 - val_acc: 0.3863\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 1.7624 - acc: 0.3601 - val_loss: 1.6862 - val_acc: 0.3878\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 1.7479 - acc: 0.3694 - val_loss: 1.7005 - val_acc: 0.3856\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 1.7378 - acc: 0.3719 - val_loss: 1.6557 - val_acc: 0.4078\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 1.7201 - acc: 0.3780 - val_loss: 1.6330 - val_acc: 0.4184\n",
      "Test loss: 1.6330318370819092\n",
      "Test accuracy: 0.4184\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 444,170\n",
      "Trainable params: 444,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# adding more layers with 128 perceptrons (still sequetial)\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(128, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 2.0577 - acc: 0.2378 - val_loss: 1.8883 - val_acc: 0.3175\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 1.8778 - acc: 0.3191 - val_loss: 1.7694 - val_acc: 0.3654\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 1.8108 - acc: 0.3454 - val_loss: 1.7356 - val_acc: 0.3837\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 1.7663 - acc: 0.3609 - val_loss: 1.7731 - val_acc: 0.3595\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 1.7326 - acc: 0.3797 - val_loss: 1.7379 - val_acc: 0.3782\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 1.7058 - acc: 0.3876 - val_loss: 1.6625 - val_acc: 0.4042\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 1.6827 - acc: 0.3952 - val_loss: 1.6154 - val_acc: 0.4234\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 1.6623 - acc: 0.4028 - val_loss: 1.6970 - val_acc: 0.4049\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 1.6472 - acc: 0.4092 - val_loss: 1.6377 - val_acc: 0.4104\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 1.6317 - acc: 0.4149 - val_loss: 1.5788 - val_acc: 0.4353\n",
      "Test loss: 1.5787645009994506\n",
      "Test accuracy: 0.4353\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple sequetial models appear to be performing quite poorly Let's try a convoluted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From C:\\Users\\Cody Martin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 270s 5ms/step - loss: 1.8500 - acc: 0.3354 - val_loss: 1.4102 - val_acc: 0.5041\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 1.4304 - acc: 0.4911 - val_loss: 1.1908 - val_acc: 0.5788\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 271s 5ms/step - loss: 1.2436 - acc: 0.5625 - val_loss: 1.1541 - val_acc: 0.5807\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 282s 6ms/step - loss: 1.1206 - acc: 0.6059 - val_loss: 1.0026 - val_acc: 0.6506\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 268s 5ms/step - loss: 1.0314 - acc: 0.6388 - val_loss: 0.9710 - val_acc: 0.6536\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 273s 5ms/step - loss: 0.9638 - acc: 0.6641 - val_loss: 0.9273 - val_acc: 0.6773\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 287s 6ms/step - loss: 0.8998 - acc: 0.6848 - val_loss: 0.9031 - val_acc: 0.6832\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 275s 5ms/step - loss: 0.8461 - acc: 0.7045 - val_loss: 0.8888 - val_acc: 0.6913\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 266s 5ms/step - loss: 0.7970 - acc: 0.7198 - val_loss: 0.9039 - val_acc: 0.6871\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 253s 5ms/step - loss: 0.7529 - acc: 0.7358 - val_loss: 0.8722 - val_acc: 0.7035\n",
      "Test loss: 0.8722326738357544\n",
      "Test accuracy: 0.7035\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 32, 32\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70.35% accuracy. Much better! However, this was (unsurprisingly) slow. Let's try a recurrent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 256s 5ms/step - loss: 2.0217 - acc: 0.2574 - val_loss: 2.0407 - val_acc: 0.2859\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 372s 7ms/step - loss: 1.8362 - acc: 0.3343 - val_loss: 1.8030 - val_acc: 0.3543\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 359s 7ms/step - loss: 1.7455 - acc: 0.3662 - val_loss: 1.8956 - val_acc: 0.3382\n",
      "Test loss: 1.8956369766235353\n",
      "Test accuracy: 0.3382\n"
     ]
    }
   ],
   "source": [
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is worse than the sequential model, but it took much longer to run. Overall the convoluted network model is probably the best, even though the run time was not the shortest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
